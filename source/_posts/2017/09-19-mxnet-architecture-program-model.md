---
title: 深度学习编程风格 (翻译)
date: 2017-09-19 14:23:30
tags:
    - mxnet
    - deeplearning
---

翻译自 https://mxnet.incubator.apache.org/architecture/program_model.html

不论最终我们多么关心性能，首先我们要做的是让代码跑起来，然后才会开始担心如何优化它。要写出整洁易读的深度学习代码还是有些挑战的，我们要做的第一件事就是搞明白编程的语法。让事情变麻烦的事情是，有那么多深度学习库，每种都有它自己的编程风格。

这篇文章中，我们的讨论集中在两个最重要的高层 (high-level) 的设计方案：

  1. 在数学计算中，是否支持符号计算还是命令式计算。
  1. 构建神经网络时，是否支持更大（更抽象）的操作还是更原子性的操作。

我们的讨论将关注于编程模型。当编程风格的设计方案对性能有影响时，我们指出来，但是不会追究实现上的细节。

<!--more-->

## 符号式编程和命令式编程

如果你是一个 Python 或 C++ 程序员，你对命令式编程应该很熟悉。命令式程序在你运行它们的时候执行计算。你的多数 Python 程序都是命令式的，就像以下 Python 代码：

```python
import numpy as np
a = np.ones(10)
b = np.ones(10) * 2
c = b * a
d = c + 1
```

当程序执行 c = b * a 时，它就在执行实际计算。

符号式编程与此不同。在符号编程风格的程序中，我们首先定义一个（可能很复杂的）函数。当定义这个函数时，实际并没有执行计算。在定义函数的过程中，我们使用占位符来代替实际的值。之后我们可以编译这个函数，并且给它实际的输入来运行它。在以下例子中，我们用符号编程来重写上面的命令式程序：

```python
A = Variable('A')
B = Variable('B')
C = B * A
D = C + Constant(1)
# compiles the function
f = compile(D)
d = f(A=np.ones(10), B=np.ones(10)*2)
```

如你所见，在符号式的版本中，当 C = B * A 这句被执行时，并没有进行实际的计算，而是生成了一个计算图 (computation graph)，也叫做符号图 (symbolic graph) 来代表这个计算。下图展示了一个计算 D 的计算图。

![computation graph](comp_graph.png)

多数符号式的程序包含一个编译的过程，可能是显式的也可能是隐式的。它把计算图转换成我们可以调用的函数。在以上例子中，数字计算只在最后一行代码中发生。符号式程序最显著的特征就是它把构建计算图和执行计算清晰地区分开来。对于神经网络，我们一般讲整个模型定义为一个计算图。

在其他的流行的深度学习框架中，Torch, Chainer 和 Minerva 使用命令式风格。符号式风格的深度学习框架包括 Theano, CGT 和 TensorFlow。我们也可以把依赖于配置文件的库，如 CXXNet 和 Caffe 也看成符号式，这时我们将配置文件的内容看成是符号图的定义。

现在你了解了这两种编程模型的不痛点，让我们比较一下两者各自的优势。

### 命令式编程更加灵活

当你在 Python 中使用命令式风格的库时，你就是在写 Python 程序。任何你可以用 Python 来完成的事情，你都可以调用命令式深度学习库中相应的函数来加速。另一方面，当你在写符号式程序是，你无法使用所有 Python 中你熟悉的用法，比如迭代。考虑如下命令式程序，想想你该如何把它翻译到符号式程序：

```python
a = 2
b = a + 1
d = np.zeros(10)
for i in range(d):
    d += np.zeros(10)
```

如果符号式 API 中没有实现 for 循环，那么这个翻译就没那么容易了。你需要使用符号式 API 定义的语言 (DSL) 来写你的程序。深度学习库中的符号式 API 是用来生成神经网络计算图的强大的 DSL。

直觉上，你可能说命令式编程比符号式编程更加自然。使用命令式编程能更容易地使用语言的特性，例如，要打印出计算过程的中间值，或是在计算流程中使用原生的控制流和循环，都更加直接。

### 符号式编程更加高效

如我们所见，命令式编程更加灵活，而且更适应语言的控制流程。那么我们为什么还需要符号式编程呢？主要的原因是效率，包括内存上的和速度上的。让我们再看一下之前的小例子。

```python
import numpy as np
a = np.ones(10)
b = np.ones(10) * 2
c = b * a
d = c + 1
...
```

![computation graph](comp_graph.png)

假设数组中的每个元素占 8 字节内存，要在 Python 命令行中运行这个程序，需要多少内存？

命令式程序中，我们需要在每一行中分配内存。这样我们分配了 4 个大小为 10 的数组，所以我们需要 4 * 10 * 8 = 320 字节。另一方面，如果我们构建一个计算图，并且事前知道我们最终只需要 d，那么我们可以重用那些分配给中间值的内存。例如，通过原地计算，我们可以重用 b 的空间来存储 c。并且我们可以重用 c 的空间来存储 d。最终我们可以把内存使用量减半，只需要 2 * 10 * 8 = 160 字节。

符号式程序受到更多限制。当我们编译 D 时，我们告诉系统只有 d 的值是需要的。计算的中间值，这里是 c，对我们是不可见的。

在符号式程序可以安全地重用内存来进行原地的计算，这对我们很有利。但是另一方面，如果我们之后需要访问 c，就不行了。所以命令式程序能更好地满足所有可能的需求。如果我们在 Python 命令行中执行命令式的程序，那么我们可以查看任何的中间变量。

符号式程序也可以做另外一种优化，叫做操作折叠（operation folding）。回到我们的小例子，其中的乘法操作和加法操作可以被合并到一个操作，如下图所示。如果我们用 GPU 来计算，仅需要一个而不是两个 GPU 核心。实际上，在优化过的库，如 CXXNet 和 Caffe 中，我们就是这样手动来优化操作的。操作折叠能够优化计算效率。

![computation graph folding](comp_graph_fold.png)

注意，在命令式程序中，你不能做操作折叠，因为中间值可能以后会被用到。在符号图中，操作折叠是可行的，因为你有整个计算图，和一个清晰的定义来说明哪些中间值是需要的，哪些是不需要的。

### 案例学习：Backprop 和自动微分

在这一节中，我们将在自动微分和反向传播中，比较两种编程模型。微分在深度学习中非常重要，因为它是我们训练模型的机制。在任何的深度学习模型中，我们都要定义一个损失函数（loss function）。损失函数告诉我们当前模型的输出和我们想要的输出的差距有多大。我们把训练集中的数据（输入和期望的输出）送入神经网络进行训练，并且更新网络参数，来使损失函数最小化。为了决定要向哪个方向更新参数，我们需要求出损失函数相对于各个参数的导数。

过去，当人们定义一个新的模型时，他们需要手动计算导数。虽然这是个直接的计算，但是对于复杂的模型，这消耗大量时间并且是个烦人的工作。所有现代的深度学习框架都提供自动微分的功能，大大简化了工程师和研究人员的工作。

命令式编程和符号式编程都可以做自动微分。我们看一下分别是怎么做的。

让我们从命令式程序开始。如下 Python 代码给我们之前的小例子做了个自动微分：

```python
class array(object) :
    """Simple Array object that support autodiff."""
    def __init__(self, value, name=None):
        self.value = value
        if name:
            self.grad = lambda g : {name : g}

    def __add__(self, other):
        assert isinstance(other, int)
        ret = array(self.value + other)
        ret.grad = lambda g : self.grad(g)
        return ret

    def __mul__(self, other):
        assert isinstance(other, array)
        ret = array(self.value * other.value)
        def grad(g):
            x = self.grad(g * other.value)
            x.update(other.grad(g * self.value))
            return x
        ret.grad = grad
        return ret

# some examples
a = array(1, 'a')
b = array(2, 'b')
c = b * a
d = c + 1
print d.value
print d.grad(1)
# Results
# 3
# {'a': 2, 'b': 1}
```

在这段代码中，每个 array 对象包含一个 grad 函数（实际上是闭包）。当然你执行 d.grad 时，它递归地调用了 grad 函数，把梯度反向传播，并返回每个输入的梯度。
