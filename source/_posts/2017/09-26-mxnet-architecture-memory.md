---
title: 优化深度学习中的内存使用
date: 2017-09-26 15:55:02
tags:
---

翻译自：https://mxnet.incubator.apache.org/architecture/note_memory.html

过去十年中，深度学习一直在往更深和更大的网络发展。尽管硬件也在快速地发展，最前沿的深度学习模型一直将 GPU 的内存用到极限。所以，我们总是想找到方法来用尽量少的内存训练更大的模型。这使我们能够训练的更快，使用更大的分批大小，从而实现更高的 GPU 利用率。

在这篇文章中，我们会探索在深度神经网络中，优化内存分配的各种技术。我们讨论了一些候选解决方案。虽然我们这里无法覆盖到所有可能的方案，但是这些都很有教育性并且足够让我们介绍设计中要面临的主要问题。

<!--more-->

## 计算图

首先让我们复习一下计算图的思想。一个计算图描述了深度学习网络中不同操作之间的依赖关系。计算图中执行的操作可以是细粒度的也可以是粗粒度的。下图展示了两个计算图的例子。

![comp_graph_example.png](./comp_graph_example.png)

计算图的概念在 Theano 和 CGT 等库中是显式地编码的。在其他库中，计算图隐式地出现在网络的配置文件中。这些库之间的主要不懂点在于它们如何计算梯度。主要有两种方式：在同一个图中进行反向传播，或者显式地表示一个反向路径来计算需要的梯度。

![back_graph.png](./back_graph.png)

像Caffe, CXXNet 和 Torch 之类的库使用前一种方式，在原始的计算图中进行反向传播。而 Theano 和 CGT 这些库使用后一种方式，显式地表示出反向路径。在我们的讨论中，我们使用后者，因为它对于优化有些优势。

然而，我们应该强调显式的反向路径方式，并不会限制我们只能在符号式库如 Theano 和 CGT 中使用它。在基于层（前向和反向操作绑定在一起的）库中，我们也可以使用显式的反向路径来计算梯度。大体上说，我们引入一个反向的节点，把它连接到前向节点中，然后在反向操作中调用 layer.backward。

![explicit_back_layer.png](./explicit_back_layer.png)

这些适用于几乎所有的深度学习库。（各个库之间存在不同，比如，告诫微分，但这不在我们的讨论范围内）

为什么显式的反向路径更好？让我们用两个例子来解释。第一个理由是显式的反向路径清晰地描述了计算之间的依赖关系。考虑以下例子，我们想要得到 A 和 B 的梯度。从图上我们可以清楚地看到，算 d(C) 的梯度的计算不依赖与 F。这意味着我们在前向计算完成后立刻释放掉 F 的内存。类似的，C 的内存也可以被回收。

![back_dep_prune.png](./back_dep_prune.png)

显式的反向路径的另一个好处是，它让我们而已有一个不同的反向路径，而不是局限于是前向路径的镜像。一个常见的例子是 split connection，如下图所示。

![back_agg_grad.png](./back_agg_grad.png)

在这个例子中，B 的输出被两个后续操作用到。如果我们想要在同一个网络中计算梯度，我们需要显式地引入一个分离层。这意味着我们在前向路径中也要有这么一个分离层。在这个图中，前向路径没有分离层，但是计算图会在把梯度传回给 B 之前，自动地插入一个梯度聚合节点。这帮助我们节省了分配分离层的输出的内存和在前向路径中复制数据的操作开销。

如果我们采用显式反向传播的方式，那么在前向路径和反向路径之间没有什么区别。我们可以简单地根据计算图，依时间顺序逐步计算。这使得显式的反向传播方式更易于分析。我们只需要回答这个问题：我们如何为计算图的每个输出节点分配内存。

## 什么可以被优化
